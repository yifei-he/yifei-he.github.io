---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Computer Science Ph.D. student at [University of Illinois Urbana-Champaign (UIUC)](https://cs.illinois.edu/), advised by [Prof. Han Zhao](https://hanzhaoml.github.io/). I obtained my dual bachelor's degree in Data Science from the [University of Michigan (UM)](https://cse.engin.umich.edu/) and in Electrical and Computer Engineering from [Shanghai Jiao Tong University (SJTU)](https://www.ji.sjtu.edu.cn/). I have also interned at Microsoft Turing, Microsoft GenAI, and Amazon Search Science & AI.

I am interested in making foundation models and agents learn from multiple sources and tasks in an efficient, robust and scalable manner. In particular, I work on 
- Multimodal agent for computer use (preprint coming soon!)
- Multi-domain learning for foundation models ([model merging](https://arxiv.org/abs/2408.13656), [multilingual LLMs](https://arxiv.org/pdf/2410.12883))
- LLM data ([reward modeling](https://arxiv.org/abs/2409.06903)) and inference efficiency ([MoE](https://arxiv.org/abs/2503.00634))

Previously, I have worked on [multi-objective optimization](https://arxiv.org/abs/2402.02009), [domain adaptation](https://arxiv.org/abs/2310.13852) and [multimodal learning](https://jmlr.org/papers/v25/23-0439.html).

**I am currently on the job market for full-time industry research scientist positions, starting summer 2026. Please feel free to reach out if my background aligns with your teamâ€™s needs!**

<h1>News</h1>

* [Aug 2025] My Microsoft GenAI internship work on [efficient MoE editing](https://arxiv.org/abs/2503.00634) is accpeted at EMNLP 2025! Check out how we compress auxiliary experts to save inference costs while maitaining performance!
* [May 2025] I return to Microsoft Turing as an applied scientist intern working on reasoning for computer use agents!
* [May 2025] My Microsoft Turing internship work [Multilingual Scaling Laws](https://arxiv.org/pdf/2410.12883) is accepted at ACL 2025! With this law, you can compute optimal sampling ratios of langauges to design your multilingual pretraining mixture for any model size!
* [Jan 2025] Our work on [mechanistic interpretability](https://arxiv.org/abs/2410.18210) is accepted at NAACL 2025!
* [Dec 2024] [Localize-and-Stitch](https://arxiv.org/abs/2408.13656) is accepted by TMLR! Check out how better localization improves model merging!
* [Sept 2024] [Semi-Supervised Reward Modeling (SSRM)](https://arxiv.org/abs/2409.06903) is accepted at EMNLP 2024!
* [Aug 2024] Start internship at Microsoft GenAI working on improve MoE efficiency!
* [May 2024] Start internship at Microsoft Turing working on multilingual scaling laws!
* [May 2024] Our work on [robust multi-task learning](https://arxiv.org/abs/2402.02009) is accepted at ICML 2024!
* [May 2024] Our work on [gradual domain adaptation](https://arxiv.org/abs/2310.13852) is accepted at JMLR! 
* [May 2023] Start internship at Amazon working on large-scale multi-task learning!
 

<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=8288ae&w=a&t=n&d=ly59zV1UzWi2oiROhB1R4JdoWHGFOp3VzSfO2zqMAPQ&co=ffffff&cmo=ff5378&cmn=ff5353&ct=ffffff"></script>    