---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a final-year Computer Science Ph.D. student at [University of Illinois Urbana-Champaign (UIUC)](https://cs.illinois.edu/), advised by [Prof. Han Zhao](https://hanzhaoml.github.io/). I obtained my dual bachelor's degree in Data Science from the [University of Michigan (UM)](https://cse.engin.umich.edu/) and in Electrical and Computer Engineering from [Shanghai Jiao Tong University (SJTU)](https://www.ji.sjtu.edu.cn/). I have also interned at Microsoft Turing, Microsoft GenAI, and Amazon Search Science & AI.

I am interested in making foundation models and agents learn from multiple sources and tasks in an efficient, robust and scalable manner. My research involves the full life cycle of LLMs:
- **Pretraining**: Deriving the optimal pretraining data mixture ([Multilingual scaling laws](https://arxiv.org/pdf/2410.12883)).
- **Posttraining**: 
  - Developing model merging techniques to efficiently combine and continually edit domain specialized LLMs ([Localize-and-Stitch](https://arxiv.org/abs/2408.13656), [MergeBench](https://arxiv.org/abs/2505.10833)).
  - Improving LLM data ([Reward modeling](https://arxiv.org/abs/2409.06903)) and inference efficiency ([MoE](https://arxiv.org/abs/2503.00634)).
  - Understanding LLM behavior through the weight space ([Mechanistic interpretability](https://arxiv.org/abs/2410.18210)).
- **Agentic AI**: Training Computer Use Agents (CUA) without human annotations ([WebSTAR](https://arxiv.org/abs/2512.10962)).

<!-- - Multimodal agent for computer use (preprint coming soon!)
- Multi-domain learning for foundation models ([model merging](https://arxiv.org/abs/2408.13656), [multilingual LLMs](https://arxiv.org/pdf/2410.12883))
- LLM data ([reward modeling](https://arxiv.org/abs/2409.06903)) and inference efficiency ([MoE](https://arxiv.org/abs/2503.00634)) -->

Previously, I have worked on [multi-objective optimization](https://arxiv.org/abs/2402.02009), [domain adaptation](https://arxiv.org/abs/2310.13852) and [multimodal learning](https://jmlr.org/papers/v25/23-0439.html).

**I am currently on the job market for full-time industry research scientist positions, starting summer 2026. Please feel free to reach out if my background aligns with your teamâ€™s needs!**

<h1>News</h1>

* [Dec 2025] [WebSTAR](https://arxiv.org/abs/2512.10962) is released! We create the first large-scale computer-use agent dataset with step-level grading and detailed reasoning. Based on that, we propose a step-level filtering for efficient, high-quality CUA training without human annotations.
* [Sept 2025] [MergeBench](https://arxiv.org/abs/2505.10833) is accepted at NeurIPS 2025! We established the first standardized benchmark for merging domain-specialized LLMs!
* [Aug 2025] My Microsoft GenAI internship work on [efficient MoE editing](https://arxiv.org/abs/2503.00634) is accpeted at EMNLP 2025! Check out how we compress auxiliary experts to save inference costs while maitaining performance!
* [May 2025] I return to Microsoft Turing as an applied scientist intern working on reasoning for computer use agents!
* [May 2025] My Microsoft Turing internship work [Multilingual Scaling Laws](https://arxiv.org/pdf/2410.12883) is accepted at ACL 2025! With this law, you can compute optimal sampling ratios of langauges to design your multilingual pretraining mixture for any model size!
* [Jan 2025] Our work on [mechanistic interpretability](https://arxiv.org/abs/2410.18210) is accepted at NAACL 2025!
* [Dec 2024] [Localize-and-Stitch](https://arxiv.org/abs/2408.13656) is accepted by TMLR! It is also awarded with TMLR J2C Certification, recognizing it as top 10% TMLR papers! Check out how better localization improves model merging!
* [Sept 2024] [Semi-Supervised Reward Modeling (SSRM)](https://arxiv.org/abs/2409.06903) is accepted at EMNLP 2024!
* [Aug 2024] Start internship at Microsoft GenAI working on improve MoE efficiency!
* [May 2024] Start internship at Microsoft Turing working on multilingual scaling laws!
* [May 2024] Our work on [Robust Multi-Task Learning](https://arxiv.org/abs/2402.02009) is accepted at ICML 2024! Check out how to learn effectively from multiple tasks even with severe label noise! 
* [May 2024] Our work on [Gradual Domain Adaptation](https://arxiv.org/abs/2310.13852) is accepted at JMLR! Check out how to generate intermediate domains useful for large distribution shift!
* [May 2023] Start internship at Amazon working on large-scale multi-task learning!
 

<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=8288ae&w=a&t=n&d=ly59zV1UzWi2oiROhB1R4JdoWHGFOp3VzSfO2zqMAPQ&co=ffffff&cmo=ff5378&cmn=ff5353&ct=ffffff"></script>    